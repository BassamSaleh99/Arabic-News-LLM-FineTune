{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgMG61_vgbHy"
      },
      "source": [
        "# Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmBzObiJIw5z"
      },
      "source": [
        "## Mount Your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HABI8KObIYIK",
        "outputId": "70b03c5d-6d0b-4c71-ebb4-05be786bcf52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1OdRcWRLEP1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OitCMS1Of5n7"
      },
      "outputs": [],
      "source": [
        "!pip install json_repair\n",
        "!pip install -qU faker==35.2.0\n",
        "!pip install -qU vllm==0.7.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhW1GjsYLJZ0"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3NP1J4rLLH-"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=userdata.get('wandb'))\n",
        "hf_token = userdata.get('huggingface')\n",
        "!huggingface-cli login --token {hf_token}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NguQUmwglzZ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tcx1rGuWfhCl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/arabic-news-llm-finetune/data/\"\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6nhIe1DJ-Xx"
      },
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssrpd51XmENp"
      },
      "outputs": [],
      "source": [
        "story = \"\"\"\n",
        "ذكرت مجلة فوربس أن العائلة تلعب دورا محوريا في تشكيل علاقة الأفراد بالمال،\n",
        " حيث تتأثر هذه العلاقة بأنماط السلوك المالي المتوارثة عبر الأجيال.\n",
        "\n",
        "التقرير الذي يستند إلى أبحاث الأستاذ الجامعي شاين إنيت حول\n",
        "الرفاه المالي يوضح أن لكل شخص \"شخصية مالية\" تتحدد وفقا لطريقة\n",
        " تفاعله مع المال، والتي تتأثر بشكل مباشر بتربية الأسرة وتجارب الطفولة.\n",
        "\n",
        " الأبعاد الثلاثة للعلاقة بالمال\n",
        "بحسب الدراسة، هناك ثلاثة أبعاد رئيسية تشكّل علاقتنا بالمال:\n",
        "\n",
        "الاكتساب (A): يميل الأفراد الذين ينتمون لهذا\n",
        " البعد إلى اعتبار المال سلعة قابلة للجمع، حيث يرون\n",
        "في تحقيق الثروة هدفا بحد ذاته. والجانب السلبي لهذا\n",
        " النمط هو إمكانية التحول إلى هوس بالثروة أو العكس،\n",
        " أي رفض تام لاكتساب المال باعتباره مصدرا للفساد.\n",
        "\n",
        "الاستخدام (U): يرى هؤلاء الأشخاص المال أداة للتمتع بالحياة، حيث يربطون قيمته بقدرته على توفير\n",
        "المتعة والراحة. ومع ذلك، قد يصبح\n",
        "البعض مدمنا على الإنفاق، في حين يتجه آخرون إلى التقشف المفرط خوفا من المستقبل.\n",
        "\n",
        "الإدارة (M): أصحاب هذا النمط يعتبرون المال مسؤولية تتطلب التخطيط الدقيق. لكن في بعض الحالات،\n",
        " قد يتحول الأمر إلى هوس مفرط بإدارة الإنفاق، مما يؤثر سلبا على العلاقات الشخصية.\n",
        "\n",
        " كيف تؤثر العائلة على علاقتنا بالمال؟\n",
        "يشير التقرير إلى أن التجارب الأسرية تلعب دورا رئيسيا في تحديد\n",
        " \"الشخصية المالية\" لكل فرد، على سبيل المثال، إذا كان أحد الوالدين يعتمد على المال\n",
        "كمكافأة للسلوك الجيد، فقد يتبنى الطفل لاحقا النمط نفسه في حياته البالغة.\n",
        "\n",
        "لتحليل هذه التأثيرات بشكل دقيق، طورت رابطة العلاج المالي\n",
        "(Financial Therapy Association) أداة تسمى مخطط الجينوم المالي (Money Genogram)،\n",
        "وهو نموذج يُستخدم لتحديد الأنماط المالية داخل العائلة.\n",
        "\n",
        "تتضمن هذه الأداة:\n",
        "\n",
        "رسم شجرة عائلية.\n",
        "تصنيف أفراد العائلة وفقا للأبعاد الثلاثة للعلاقة بالمال (A ،U ،M).\n",
        "تحديد ما إذا كان السلوك المالي لكل فرد صحيا (+) أو غير صحي (-).\n",
        "على سبيل المثال، إذا نشأ شخص في عائلة\n",
        "اعتادت على الإنفاق المفرط، فقد يكون لديه ميل قوي إلى اتباع النمط نفسه،\n",
        " أو العكس تماما، حيث يصبح مقتصدا بشكل مبالغ فيه كرد فعل نفسي.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M5x3sgKtpZxv"
      },
      "outputs": [],
      "source": [
        "# # # Another Sample\n",
        "\n",
        "# story = \"\"\"\n",
        "# قرر المجلس القومي للأجور في مصر، زيادة الحد الأدنى لأجر العاملين بالقطاع الخاص إلى 7 آلاف جنيه شهريًا مقابل 6 آلاف جنيه، على أن يتم تطبيق الزيادة اعتبارًا من 1 مارس 2025.\n",
        "# كما قرر المجلس أن يكون الحد الأدنى لقيمة العلاوة الدورية للعاملين بالقطاع الخاص 250 جنيهًا شهريًا، ولأول مرة يقرر المجلس القومي للأجور وضع حد أدنى للأجر للعمل المؤقت \"جزء من الوقت\"، بحيث لا يقل أجرهم عن 28 جنيهًا صافيًا في الساعة، وذلك وفقًا لتعريفهم الوارد في قانون العمل.\n",
        "# وقالت وزيرة التخطيط والتنمية الاقتصادية والتعاون الدولي، رانيا المشاط، إن رفع الحد الأدنى للأجور يأتي في إطار الحرص على الاستجابة للمستجدات الاقتصادية الراهنة، بما يعزز الاستقرار الاقتصادي والاجتماعي، مضيفة أن ذلك يتسق مع المعايير الدولية، حيث تؤكد منظمة العمل الدولية على ضرورة مراجعة الحد الأدنى للأجور على أساس دوري، لحماية القوة الشرائية للأسر، واستيعاب التغيرات الاقتصادية التدريجية.\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_ATOR6MKIt6"
      },
      "source": [
        "### Details Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha5mckQVMXDa"
      },
      "outputs": [],
      "source": [
        "# {\n",
        "#     \"story_title\": \"\",\n",
        "#     \"story_keywords\": [\"kw1\",\"kw2\"],\n",
        "#     \"story_summary\":  [\"....\", \",,,,\"],\n",
        "#     \"story_category\": \"\",\n",
        "#     \"story_entities\": [\n",
        "#     {\n",
        "#         \"entity_value\": \"\",\n",
        "#         \"entity_type\": \"\"\n",
        "#     }\n",
        "# ]\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0TYzMnmhKIhy"
      },
      "outputs": [],
      "source": [
        "StoryCategory = Literal[\n",
        "    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "    \"health\", \"entertainment\", \"science\",\n",
        "    \"not_specified\"\n",
        "]\n",
        "\n",
        "EntityType = Literal[\n",
        "    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n",
        "    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n",
        "    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"A fully informative and SEO optimized title of the story.\")\n",
        "\n",
        "    story_keywords: List[str] = Field(..., min_items=1,\n",
        "                                      description=\"Relevant keywords associated with the story.\")\n",
        "\n",
        "    story_summary: List[str] = Field(\n",
        "                                    ..., min_items=1, max_items=5,\n",
        "                                    description=\"Summarized key points about the story (1-5 points).\"\n",
        "                                )\n",
        "\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n",
        "\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n",
        "                                        description=\"List of identified entities in the story.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wi4A5rSFKCRP"
      },
      "outputs": [],
      "source": [
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps(\n",
        "                NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "            ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5LiqPqXkNyS"
      },
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctW052M1kNYr"
      },
      "outputs": [],
      "source": [
        "# {\n",
        "#     \"translated_title\": \"\",\n",
        "#     \"translated_content\": \"\"\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KcA44Jpgkqk-"
      },
      "outputs": [],
      "source": [
        "class TranslatedStory(BaseModel):\n",
        "    translated_title: str = Field(..., min_length=5, max_length=300,\n",
        "                                  description=\"Suggested translated title of the news story.\")\n",
        "    translated_content: str = Field(..., min_length=5,\n",
        "                                    description=\"Translated content of the news story.\")\n",
        "\n",
        "targeted_lang = \"English\"\n",
        "\n",
        "translation_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are a professional translator.\",\n",
        "            \"You will be provided by an Arabic text.\",\n",
        "            \"You have to translate the text into the `Targeted Language`.\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":  \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Targeted Language:\",\n",
        "            targeted_lang,\n",
        "            \"\",\n",
        "\n",
        "            \"## Translated Story:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ])\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXZ2vxTnWSqa"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVlIbVpIWFEv"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W9X954HZPG2",
        "outputId": "355c8345-46d6-4574-b2c5-5cd4d856e3ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vzqAQf0beBR",
        "outputId": "5148f3cd-9915-41ed-905b-f924e7e3645b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"story_title\": \"How Family Influences Financial Behavior\",\n",
            "  \"story_keywords\": [\n",
            "    \"family influence\",\n",
            "    \"financial behavior\",\n",
            "    \"moneymaking\",\n",
            "    \"money management\",\n",
            "    \"personal finance\"\n",
            "  ],\n",
            "  \"story_summary\": [\n",
            "    \"Family plays a crucial role in shaping individuals' financial relationships.\",\n",
            "    \"Individuals inherit certain financial behaviors through their families.\"\n",
            "  ],\n",
            "  \"story_category\": \"economy\",\n",
            "  \"story_entities\": [\n",
            "    {\n",
            "      \"entity_value\": \"Families\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"Financial Therapists\",\n",
            "      \"entity_type\": \"organization\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1q7Itsxje19"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUXfRztUlw2x",
        "outputId": "23ffb402-632a-40c3-c863-b11b3ad60560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"translated_title\": \"Forbes Magazine Reveals Family Plays a Central Role in Forming Individuals' Financial Relationships\",\n",
            "  \"translated_content\": \"According to Forbes magazine, family plays a crucial role in shaping individuals' financial relationships, as these relationships are influenced by inherited behavioral patterns across generations.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlTHtpVgne2T"
      },
      "source": [
        "## Evaluate OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LvgsoRLnes1"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    api_key=userdata.get('openai-colab'),\n",
        "    # base_url=\"http://localhost:8000\"\n",
        ")\n",
        "\n",
        "openai_model_id = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICi3emtDnemb"
      },
      "outputs": [],
      "source": [
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=details_extraction_messages,\n",
        "    model=openai_model_id,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET2_BnrRni-r"
      },
      "outputs": [],
      "source": [
        "chat_completion = openai_client.chat.completions.create(\n",
        "    messages=translation_messages,\n",
        "    model=openai_model_id,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pa87wphnkfr"
      },
      "outputs": [],
      "source": [
        "json.loads(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qJSdM2mlQl5"
      },
      "source": [
        "## Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2i7kcwxn2Ne"
      },
      "outputs": [],
      "source": [
        "raw_data_path = join(data_dir, \"news-sample.jsonl\")\n",
        "\n",
        "raw_data = []\n",
        "for line in open(raw_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    raw_data.append(\n",
        "        json.loads(line.strip())\n",
        "    )\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "\n",
        "print(f\"Raw data: {len(raw_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKKk90Kbn2AY"
      },
      "outputs": [],
      "source": [
        "raw_data[0]['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXGfC6KSr-AT"
      },
      "outputs": [],
      "source": [
        "cloud_model_id = \"gpt-4o-mini\"\n",
        "price_per_1m_input_tokens = 0.150\n",
        "price_per_1m_output_tokens = 0.600\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "save_to = join(data_dir, \"sft.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "\n",
        "    sample_details_extraction_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are an NLP data paraser.\",\n",
        "                \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "                \"Generate the ouptut in the same story language.\",\n",
        "                \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "                \"Extract details as mentioned in text.\",\n",
        "                \"Do not generate any introduction or conclusion.\"\n",
        "            ])\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"## Story:\",\n",
        "                story['content'].strip(),\n",
        "                \"\",\n",
        "\n",
        "                \"## Pydantic Details:\",\n",
        "                json.dumps(\n",
        "                    NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "                ),\n",
        "                \"\",\n",
        "\n",
        "                \"## Story Details:\",\n",
        "                \"```json\"\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "                            messages=sample_details_extraction_messages,\n",
        "                            model=cloud_model_id,\n",
        "                            temperature=0.2,\n",
        "                        )\n",
        "\n",
        "    if response.choices[0].finish_reason != \"stop\":\n",
        "        prompt_tokens += response.usage.prompt_tokens\n",
        "        continue\n",
        "\n",
        "    llm_response = response.choices[0].message.content\n",
        "    llm_resp_dict = parse_json(llm_response)\n",
        "\n",
        "    if not llm_resp_dict:\n",
        "        continue\n",
        "\n",
        "    with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"id\": ix,\n",
        "            \"story\": story['content'].strip(),\n",
        "            \"task\": \"Extrat the story details into a JSON.\",\n",
        "            \"output_scheme\": json.dumps( NewsDetails.model_json_schema(), ensure_ascii=False ),\n",
        "            \"response\": llm_resp_dict,\n",
        "        }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "    ix += 1\n",
        "    prompt_tokens += response.usage.prompt_tokens\n",
        "    completion_tokens += response.usage.completion_tokens\n",
        "\n",
        "    if(ix % 3) == 0:\n",
        "        cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "        cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "        total_cost = cost_input + cost_output\n",
        "\n",
        "        print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvyzXLTbvlmT"
      },
      "outputs": [],
      "source": [
        "cloud_model_id = \"gpt-4o-mini\"\n",
        "price_per_1m_input_tokens = 0.150\n",
        "price_per_1m_output_tokens = 0.600\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "save_to = join(data_dir, \"sft.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "\n",
        "    for targeted_lang in [\"English\", \"French\"]:\n",
        "        sample_translation_messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\\n\".join([\n",
        "                    \"You are a professional translator.\",\n",
        "                    \"You will be provided by an Arabic text.\",\n",
        "                    \"You have to translate the text into the `Targeted Language`.\",\n",
        "                    \"Follow the provided Scheme to generate a JSON\",\n",
        "                    \"Do not generate any introduction or conclusion.\"\n",
        "                ])\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\\n\".join([\n",
        "                    \"## Pydantic Details:\",\n",
        "                    json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Targeted Language or Dialect:\",\n",
        "                    targeted_lang,\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Story:\",\n",
        "                    story['content'].strip(),\n",
        "                    \"\",\n",
        "\n",
        "                    \"## Translated Story:\",\n",
        "                    \"```json\"\n",
        "                ])\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        response = openai_client.chat.completions.create(\n",
        "                                messages=sample_translation_messages,\n",
        "                                model=cloud_model_id,\n",
        "                                temperature=0.2,\n",
        "                            )\n",
        "\n",
        "        if response.choices[0].finish_reason != \"stop\":\n",
        "            prompt_tokens += response.usage.prompt_tokens\n",
        "            continue\n",
        "\n",
        "        llm_response = response.choices[0].message.content\n",
        "        llm_resp_dict = parse_json(llm_response)\n",
        "\n",
        "        if not llm_resp_dict:\n",
        "            continue\n",
        "\n",
        "        with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "            dest.write(json.dumps({\n",
        "                \"id\": ix,\n",
        "                \"story\": story['content'].strip(),\n",
        "\n",
        "                \"output_scheme\": json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "                \"task\": f\"You have to translate the story content into {targeted_lang} associated with a title into a JSON.\",\n",
        "\n",
        "                \"response\": llm_resp_dict,\n",
        "            }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "        ix += 1\n",
        "        prompt_tokens += response.usage.prompt_tokens\n",
        "        completion_tokens += response.usage.completion_tokens\n",
        "\n",
        "        if(ix % 3) == 0:\n",
        "            cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "            cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "            total_cost = cost_input + cost_output\n",
        "\n",
        "            print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6XlmnAKv90m"
      },
      "source": [
        "## Format Finetuning Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "runNSoL7voBb"
      },
      "outputs": [],
      "source": [
        "\n",
        "sft_data_path = join(data_dir, \"sft.jsonl\")\n",
        "llm_finetunning_data = []\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join([\n",
        "            \"# Story:\",\n",
        "            rec[\"story\"],\n",
        "\n",
        "            \"# Task:\",\n",
        "            rec[\"task\"],\n",
        "\n",
        "            \"# Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"# Output JSON:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ]),\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"\\n\".join([\n",
        "            \"```json\",\n",
        "            json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "            \"```\"\n",
        "        ]),\n",
        "        \"history\": []\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetunning_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y7SI1KGdLUQ"
      },
      "outputs": [],
      "source": [
        "train_sample_sz = 2700\n",
        "\n",
        "train_ds = llm_finetunning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetunning_data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(data_dir, \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(data_dir, \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(data_dir, \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYjjuOp1iKIB"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpxrwdqsiJpC"
      },
      "outputs": [],
      "source": [
        "# # Configure LLaMA-Factory for the new datasets\n",
        "\n",
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "# ```\n",
        "   \"news_finetune_train\": {\n",
        "        \"file_name\": \"/drive/MyDrive/arabic-news-llm-finetune/data/llamafactory-finetune-data/train.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"news_finetune_val\": {\n",
        "        \"file_name\": \"/drive/MyDrive/arabic-news-llm-finetune/data/llamafactory-finetune-data/val.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }\n",
        "# ```\n",
        "\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/apwbkni9\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/c5tf0q90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbepqdxBkrUr"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "# resume_from_checkpoint: /drive/MyDrive/arabic-news-llm-finetune/models/checkpoint-1500\n",
        "output_dir: /drive/MyDrive/arabic-news-llm-finetune/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: arabic-news-llm-finetune\n",
        "\n",
        "# push_to_hub: true\n",
        "# export_hub_model_id: \"\"\n",
        "# hub_private_repo: true\n",
        "# hub_strategy: checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYAlYYLitoHx"
      },
      "outputs": [],
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcIHBIn4q8hx"
      },
      "source": [
        "## New FinetuAned Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnNI_OtNvCyy"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3le5ZwyASvFH"
      },
      "outputs": [],
      "source": [
        "finetuned_model_id = \"/content/drive/MyDrive/arabic-news-llm-finetune/models\"\n",
        "model.load_adapter(finetuned_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX-ba3SaS4NP",
        "outputId": "09bd0a0f-e3ef-4cd8-873f-7c81f91ef5b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ],
      "source": [
        "def generate_resp(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "response = generate_resp(translation_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRG9SBktS7Ng",
        "outputId": "e4af786a-bb19-4c0f-896d-17c54bd1052c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'translated_title': \"Egypt's Labor Council Raises Minimum Wage for Private Sector Workers\",\n",
              " 'translated_content': 'The Egyptian National Labor Council has decided to increase the minimum wage for private sector workers from 6,000 Egyptian pounds per month to 7,000 pounds starting March 2025. The council also set the minimum value for periodic bonuses for private sector workers at 250 pounds per month for the first time, establishing a minimum wage for temporary work \"part-time\" that must not fall below 28 pounds net per hour, according to their definition under the Labor Law. Minister of Planning and Economic Development and International Cooperation, Rania Maktaba, stated that raising the minimum wage is in line with responding to current economic developments, aiming to enhance economic stability and social harmony, adding that this aligns with international standards, as the International Labor Organization emphasizes the need to regularly review the minimum wage to protect household purchasing power and accommodate gradual economic changes.'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parse_json(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALRqnkshsKdC"
      },
      "source": [
        "## Cost Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfC4siLjViRw"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "start_time = datetime.now()\n",
        "fake = Faker('ar')\n",
        "\n",
        "input_tokens = 0\n",
        "output_tokens = 0\n",
        "\n",
        "for i in tqdm(range(30)):\n",
        "    prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = generate_resp(messages)\n",
        "\n",
        "    input_tokens += len(tokenizer.apply_chat_template(messages))\n",
        "    output_tokens += len(tokenizer.encode(response))\n",
        "\n",
        "total_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"Total Time: {total_time} seconds\")\n",
        "print(f\"Input Tokens: {input_tokens}\")\n",
        "print(f\"Output Tokens: {output_tokens}\")\n",
        "print(f\"Total Tokens: {input_tokens + output_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZHako_ClsQSI"
      },
      "outputs": [],
      "source": [
        "# Total Time: 387.838115 seconds\n",
        "# Input Tokens: 2446\n",
        "# Output Tokens: 7375\n",
        "# Total Tokens: 9821"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEzjOUKasuL3",
        "outputId": "42941062-25d9-4862-f6e9-6d2a1d8c0c03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25.311855670103093"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "9821  / 388"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nThvuFxatJ84"
      },
      "source": [
        "## vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGkidUPesv6p",
        "outputId": "e44b1b62-159e-48ce-c5a4-b1522f088745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ],
      "source": [
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "adapter_model_id = \"/content/drive/MyDrive/arabic-news-llm-finetune/models\"\n",
        "\n",
        "\n",
        "!nohup vllm serve \"{base_model_id}\" --dtype=half --gpu-memory-utilization 0.8 --max_lora_rank 64 --enable-lora --lora-modules news-lora=\"{adapter_model_id}\" &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKU_IcsNthDL"
      },
      "outputs": [],
      "source": [
        "!tail -n 30 nohup.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU9Qldc-vkUQ"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kpaR4TpJvN1C"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR3am1K9vmPq"
      },
      "outputs": [],
      "source": [
        "vllm_model_id = \"news-lora\"\n",
        "\n",
        "llm_response = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
        "    \"model\": vllm_model_id,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.3\n",
        "})\n",
        "\n",
        "llm_response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v7zLEAMwH1s"
      },
      "source": [
        "## Load Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4QWR_byvn21"
      },
      "outputs": [],
      "source": [
        "%%writefile locust.py\n",
        "\n",
        "import random\n",
        "import json\n",
        "from locust import HttpUser, task, between, constant\n",
        "from transformers import AutoTokenizer\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker('ar')\n",
        "\n",
        "class CompletionLoadTest(HttpUser):\n",
        "    wait_time = between(1, 3)\n",
        "\n",
        "    @task\n",
        "    def post_completion(self):\n",
        "        model_id = \"news-lora\"\n",
        "        prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "        message = {\n",
        "            \"model\": model_id,\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": 512,\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "\n",
        "        llm_response = self.client.post(\"/v1/completions\", json=message)\n",
        "\n",
        "        if llm_response.status_code == 200:\n",
        "            with open(\"./vllm_tokens.txt\", \"a\") as dest:\n",
        "                dest.write(json.dumps({\n",
        "                    \"prompt\": prompt,\n",
        "                    \"response\": llm_response.json()[\"choices\"][0][\"text\"],\n",
        "                }, ensure_ascii=False) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5d9Fa7FwLzU"
      },
      "outputs": [],
      "source": [
        "!locust --headless -f locust.py --host=http://localhost:8000 -u 20 -r 1 -t \"60s\" --html=locust_results.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOMF92jQwODK"
      },
      "outputs": [],
      "source": [
        "vllm_tokens = [\n",
        "    json.loads(line.strip())\n",
        "    for line in open(\"./vllm_tokens.txt\") if line.strip() != \"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYlokUxvwPpa"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "total_input_tokens = sum([ len(tokenizer.encode(rec['prompt'])) for rec in vllm_tokens ])\n",
        "total_output_tokens = sum([ len(tokenizer.encode(rec['response'])) for rec in vllm_tokens ])\n",
        "\n",
        "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
        "print(f\"Total Output Tokens: {total_output_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_3og2mwl7n",
        "outputId": "717432df-ae29-4670-fd8d-bbd452371260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "630.6666666666666"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "37840 / 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSgstTelwnT3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
